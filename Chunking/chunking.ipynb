{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58a7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- imports & setup --------------------\n",
    "import os, re, json, time, math, statistics\n",
    "from typing import List, Dict, Tuple\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sentence/paragraph + similarity merges\n",
    "# import nltk\n",
    "# try:\n",
    "#     nltk.data.find(\"tokenizers/punkt\")\n",
    "# except LookupError:\n",
    "#     nltk.download(\"punkt\", quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# light-weight similarity for semantic merges\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Gemini (LLM-based chunking)\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Playwright (for structural HTML fallback on SPA)\n",
    "from playwright.sync_api import sync_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "795f3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Setup\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(r\"C:\\\\Users\\\\harsh\\\\OneDrive\\\\Desktop\\\\LLM Assignment 2\\\\Chunking\")\n",
    "SECTIONS_JSON = Path(r\"C:\\\\Users\\\\harsh\\\\OneDrive\\\\Desktop\\\\LLM Assignment 2\\\\Scraping\\\\data\\\\jiopay_sections.json\")\n",
    "OUT_DIR = Path(r\"C:\\\\Users\\\\harsh\\\\OneDrive\\\\Desktop\\\\LLM Assignment 2\\\\Chunking\\\\chunks\")\n",
    "ABLATION_CSV = Path(r\"C:\\\\Users\\\\harsh\\\\OneDrive\\\\Desktop\\\\LLM Assignment 2\\\\Chunking\\\\chunking_ablation.csv\")\n",
    "\n",
    "load_dotenv()\n",
    "# print(os.getenv(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87335f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_count(text: str) -> int:\n",
    "    return len(tokenizer.encode(text or \"\", add_special_tokens=False))\n",
    "\n",
    "# -------------------- structural HTML fetch --------------------\n",
    "def _looks_like_spa_shell(html: str) -> bool:\n",
    "    if not html: return True\n",
    "    text = \" \".join(BeautifulSoup(html, \"lxml\").stripped_strings)[:400].lower()\n",
    "    return (\"enable javascript\" in text) or (html.count(\"<h1\") + html.count(\"<h2\") + html.count(\"<h3\") < 1)\n",
    "\n",
    "def fetch_html_via_requests(url: str) -> str:\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "_pw = None  # cache a browser across calls\n",
    "def fetch_html_via_playwright(url: str) -> str:\n",
    "    global _pw\n",
    "    try:\n",
    "        if _pw is None:\n",
    "            _pw = sync_playwright().start()\n",
    "            _pw.browser = _pw.chromium.launch(headless=True)\n",
    "            _pw.page = _pw.browser.new_page(user_agent=\"Mozilla/5.0\")\n",
    "        _pw.page.goto(url, wait_until=\"networkidle\", timeout=60000)\n",
    "        # gentle scroll to trigger lazy content\n",
    "        try:\n",
    "            _pw.page.evaluate(\"\"\"async () => {\n",
    "                let h=document.body.scrollHeight, y=0;\n",
    "                while (y<h){ y+=Math.max(300, Math.floor(window.innerHeight*0.9));\n",
    "                  window.scrollTo(0,y); await new Promise(r=>setTimeout(r,80)); h=document.body.scrollHeight;}\n",
    "            }\"\"\")\n",
    "        except: pass\n",
    "        return _pw.page.content()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def fetch_structural_html(url: str) -> str:\n",
    "    html = fetch_html_via_requests(url)\n",
    "    if _looks_like_spa_shell(html):\n",
    "        html = fetch_html_via_playwright(url)\n",
    "    return html or \"\"\n",
    "\n",
    "# -------------------- structural chunking (preserve hierarchy) --------------------\n",
    "def structural_chunks_from_html(html: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build chunks by heading hierarchy. We preserve H1/H2/H3 context.\n",
    "    Each chunk = \"H1 > H2 > H3\\nparagraph block\"\n",
    "    \"\"\"\n",
    "    if not html: return []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for el in soup([\"script\",\"style\",\"noscript\",\"svg\",\"header\",\"footer\",\"nav\"]):\n",
    "        el.decompose()\n",
    "\n",
    "    elems = soup.find_all([\"h1\",\"h2\",\"h3\",\"p\",\"li\"], recursive=True)\n",
    "    h = {1: None, 2: None, 3: None}\n",
    "    buf, chunks = [], []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal buf\n",
    "        if buf:\n",
    "            heading_path = \" > \".join([x for x in [h[1], h[2], h[3]] if x])\n",
    "            prefix = (heading_path + \"\\n\") if heading_path else \"\"\n",
    "            chunks.append(prefix + \" \".join(buf))\n",
    "            buf = []\n",
    "\n",
    "    for el in elems:\n",
    "        txt = el.get_text(\" \", strip=True)\n",
    "        if not txt: continue\n",
    "        if el.name in (\"h1\",\"h2\",\"h3\"):\n",
    "            flush()\n",
    "            lvl = int(el.name[1])\n",
    "            h[lvl] = txt\n",
    "            for k in range(lvl+1,4): h[k]=None\n",
    "        else:\n",
    "            buf.append(txt)\n",
    "    flush()\n",
    "    # drop tiny whitespace-only chunks\n",
    "    chunks = [c.strip() for c in chunks if tok_count(c.strip()) > 0]\n",
    "    return chunks\n",
    "\n",
    "# -------------------- fixed chunking --------------------\n",
    "FIXED_SIZES = [256, 512, 1024]\n",
    "FIXED_OVERLAPS = [0, 64, 128]\n",
    "\n",
    "def fixed_chunks(text: str, size: int, overlap: int) -> List[str]:\n",
    "    ids = tokenizer.encode(text or \"\", add_special_tokens=False)\n",
    "    chunks = []\n",
    "    if not ids: return chunks\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        part = ids[i:i+size]\n",
    "        chunks.append(tokenizer.decode(part))\n",
    "        step = max(1, size - overlap)\n",
    "        i += step\n",
    "    return chunks\n",
    "\n",
    "# -------------------- semantic chunking (sentence/paragraph + sim merges) --------------------\n",
    "def _sentences(text: str) -> List[str]:\n",
    "    try:\n",
    "        return sent_tokenize(text)\n",
    "    except LookupError:\n",
    "        # fallback: crude regex\n",
    "        return re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "def _merge_by_similarity(units: List[str], target_tokens=512, sim_threshold=0.25) -> List[str]:\n",
    "    if not units: return []\n",
    "    chunks, cur = [], []\n",
    "    vec = TfidfVectorizer(min_df=1, stop_words=None).fit(units)\n",
    "    for u in units:\n",
    "        if not cur:\n",
    "            cur.append(u); continue\n",
    "        # check size\n",
    "        if tok_count(\" \".join(cur + [u])) <= target_tokens:\n",
    "            # similarity with last sentence/paragraph\n",
    "            A = vec.transform([cur[-1]])\n",
    "            B = vec.transform([u])\n",
    "            sim = float(cosine_similarity(A, B)[0][0])\n",
    "            if sim >= sim_threshold:\n",
    "                cur.append(u)\n",
    "            else:\n",
    "                chunks.append(\" \".join(cur)); cur=[u]\n",
    "        else:\n",
    "            chunks.append(\" \".join(cur)); cur=[u]\n",
    "    if cur: chunks.append(\" \".join(cur))\n",
    "    return chunks\n",
    "\n",
    "def semantic_sentence_chunks(text: str, target_tokens=512, sim_threshold=0.25) -> List[str]:\n",
    "    units = _sentences(text or \"\")\n",
    "    return _merge_by_similarity(units, target_tokens, sim_threshold)\n",
    "\n",
    "def semantic_paragraph_chunks(text: str, target_tokens=512, sim_threshold=0.20) -> List[str]:\n",
    "    # split paragraphs on 2+ newlines as a light heuristic\n",
    "    paras = [p.strip() for p in re.split(r'\\n\\s*\\n+', text or \"\") if p.strip()]\n",
    "    if not paras:\n",
    "        paras = [text] if text else []\n",
    "    return _merge_by_similarity(paras, target_tokens, sim_threshold)\n",
    "\n",
    "# -------------------- recursive chunking --------------------\n",
    "def recursive_chunks(text: str, max_tokens=512) -> List[str]:\n",
    "    \"\"\"structural -> semantic -> fixed\"\"\"\n",
    "    # try structural on text converted to pseudo-HTML? better: just semantic if we only have text\n",
    "    # Here: we do semantic sentences first; any oversize becomes fixed.\n",
    "    out = []\n",
    "    for s in semantic_sentence_chunks(text, target_tokens=max_tokens):\n",
    "        if tok_count(s) <= max_tokens:\n",
    "            out.append(s)\n",
    "        else:\n",
    "            out.extend(fixed_chunks(s, size=max_tokens, overlap=64))\n",
    "    return out\n",
    "\n",
    "# -------------------- LLM-based chunking (Gemini) --------------------\n",
    "def gemini_chunk(text: str, target_tokens=300, model=\"gemini-1.5-flash\") -> List[str]:\n",
    "    prompt = f\"\"\"You are a professional technical editor.\n",
    "Split the following text into coherent chunks of about {target_tokens} tokens each.\n",
    "Each chunk should contain a single topical unit. Return ONLY valid JSON: a list of strings.\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        resp = genai.GenerativeModel(model).generate_content(prompt)\n",
    "        out = resp.text.strip()\n",
    "        if out.startswith(\"```\"):\n",
    "            out = re.sub(r\"^```(json)?\", \"\", out, flags=re.M).strip(\"` \\n\")\n",
    "        parsed = json.loads(out)\n",
    "        chunks = [c for c in parsed if isinstance(c, str) and c.strip()]\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(\"Gemini chunking failed:\", e)\n",
    "        return []\n",
    "\n",
    "# -------------------- run all strategies & save --------------------\n",
    "def save_jsonl(path: Path, rows: List[Dict]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def run_all_chunking():\n",
    "    docs = json.loads(SECTIONS_JSON.read_text(encoding=\"utf-8\"))\n",
    "    ablation_rows = []\n",
    "\n",
    "    # --- 1) FIXED (9 configs) ---\n",
    "    for size in FIXED_SIZES:\n",
    "        for overlap in FIXED_OVERLAPS:\n",
    "            cfg_name = f\"fixed_s{size}_o{overlap}\"\n",
    "            rows, t0 = [], time.time()\n",
    "            for d in tqdm(docs, desc=cfg_name):\n",
    "                url, section, text = d.get(\"url\",\"\"), d.get(\"section\",\"\"), d.get(\"text\",\"\")\n",
    "                if not text: continue\n",
    "                for ch in fixed_chunks(text, size=size, overlap=overlap):\n",
    "                    rows.append({\"strategy\":\"fixed\", \"config\":cfg_name, \"url\":url, \"section\":section,\n",
    "                                 \"text\":ch, \"tokens\":tok_count(ch)})\n",
    "            elapsed = time.time()-t0\n",
    "            save_jsonl(OUT_DIR / f\"chunks_{cfg_name}.jsonl\", rows)\n",
    "            if rows:\n",
    "                toks = [r[\"tokens\"] for r in rows]\n",
    "                ablation_rows.append({\n",
    "                    \"strategy\":\"fixed\", \"config\":cfg_name,\n",
    "                    \"#chunks\": len(rows),\n",
    "                    \"tokens_total\": sum(toks),\n",
    "                    \"avg_tokens\": round(statistics.mean(toks),2),\n",
    "                    \"std_tokens\": round(statistics.pstdev(toks),2) if len(toks)>1 else 0.0,\n",
    "                    \"time_sec\": round(elapsed,2),\n",
    "                    \"redundancy_pct\": round((1 - max(1, size - overlap)/size)*100, 2) if overlap>0 else 0.0\n",
    "                })\n",
    "            else:\n",
    "                ablation_rows.append({\"strategy\":\"fixed\",\"config\":cfg_name,\"#chunks\":0,\"tokens_total\":0,\n",
    "                                      \"avg_tokens\":0,\"std_tokens\":0,\"time_sec\":round(elapsed,2),\n",
    "                                      \"redundancy_pct\":0.0})\n",
    "\n",
    "    # --- 2) SEMANTIC (sentence + paragraph) ---\n",
    "    for mode in [\"sentence\",\"paragraph\"]:\n",
    "        cfg_name = f\"semantic_{mode}_t512\"\n",
    "        rows, t0 = [], time.time()\n",
    "        for d in tqdm(docs, desc=cfg_name):\n",
    "            url, section, text = d.get(\"url\",\"\"), d.get(\"section\",\"\"), d.get(\"text\",\"\")\n",
    "            if not text: continue\n",
    "            if mode == \"sentence\":\n",
    "                chunks = semantic_sentence_chunks(text, target_tokens=512, sim_threshold=0.25)\n",
    "            else:\n",
    "                chunks = semantic_paragraph_chunks(text, target_tokens=512, sim_threshold=0.20)\n",
    "            for ch in chunks:\n",
    "                rows.append({\"strategy\":\"semantic\", \"config\":cfg_name, \"url\":url, \"section\":section,\n",
    "                             \"text\":ch, \"tokens\":tok_count(ch)})\n",
    "        elapsed = time.time()-t0\n",
    "        save_jsonl(OUT_DIR / f\"chunks_{cfg_name}.jsonl\", rows)\n",
    "        toks = [r[\"tokens\"] for r in rows] if rows else []\n",
    "        ablation_rows.append({\n",
    "            \"strategy\":\"semantic\", \"config\":cfg_name,\n",
    "            \"#chunks\": len(rows), \"tokens_total\": sum(toks) if toks else 0,\n",
    "            \"avg_tokens\": round(statistics.mean(toks),2) if toks else 0,\n",
    "            \"std_tokens\": round(statistics.pstdev(toks),2) if len(toks)>1 else 0,\n",
    "            \"time_sec\": round(elapsed,2), \"redundancy_pct\": 0.0\n",
    "        })\n",
    "\n",
    "    # --- 3) STRUCTURAL (fetch real HTML; preserve headings) ---\n",
    "    cfg_name = \"structural_html\"\n",
    "    rows, t0 = [], time.time()\n",
    "    for d in tqdm(docs, desc=cfg_name):\n",
    "        url, section = d.get(\"url\",\"\"), d.get(\"section\",\"\")\n",
    "        html = fetch_structural_html(url)\n",
    "        chunks = structural_chunks_from_html(html)\n",
    "        for ch in chunks:\n",
    "            rows.append({\"strategy\":\"structural\",\"config\":cfg_name,\"url\":url,\"section\":section,\n",
    "                         \"text\":ch, \"tokens\":tok_count(ch)})\n",
    "    elapsed = time.time()-t0\n",
    "    save_jsonl(OUT_DIR / f\"chunks_{cfg_name}.jsonl\", rows)\n",
    "    toks = [r[\"tokens\"] for r in rows] if rows else []\n",
    "    ablation_rows.append({\n",
    "        \"strategy\":\"structural\",\"config\":cfg_name,\n",
    "        \"#chunks\": len(rows), \"tokens_total\": sum(toks) if toks else 0,\n",
    "        \"avg_tokens\": round(statistics.mean(toks),2) if toks else 0,\n",
    "        \"std_tokens\": round(statistics.pstdev(toks),2) if len(toks)>1 else 0,\n",
    "        \"time_sec\": round(elapsed,2), \"redundancy_pct\": 0.0\n",
    "    })\n",
    "\n",
    "    # --- 4) RECURSIVE (structural -> semantic -> fixed) ---\n",
    "    cfg_name = \"recursive_t512\"\n",
    "    rows, t0 = [], time.time()\n",
    "    for d in tqdm(docs, desc=cfg_name):\n",
    "        url, section = d.get(\"url\",\"\"), d.get(\"section\",\"\")\n",
    "        # Start from structural blocks; if empty, fallback to the text field\n",
    "        html = fetch_structural_html(url)\n",
    "        base_blocks = structural_chunks_from_html(html)\n",
    "        if not base_blocks:\n",
    "            base_blocks = [d.get(\"text\",\"\")]\n",
    "        for block in base_blocks:\n",
    "            for ch in recursive_chunks(block, max_tokens=512):\n",
    "                rows.append({\"strategy\":\"recursive\",\"config\":cfg_name,\"url\":url,\"section\":section,\n",
    "                             \"text\":ch, \"tokens\":tok_count(ch)})\n",
    "    elapsed = time.time()-t0\n",
    "    save_jsonl(OUT_DIR / f\"chunks_{cfg_name}.jsonl\", rows)\n",
    "    toks = [r[\"tokens\"] for r in rows] if rows else []\n",
    "    ablation_rows.append({\n",
    "        \"strategy\":\"recursive\",\"config\":cfg_name,\n",
    "        \"#chunks\": len(rows), \"tokens_total\": sum(toks) if toks else 0,\n",
    "        \"avg_tokens\": round(statistics.mean(toks),2) if toks else 0,\n",
    "        \"std_tokens\": round(statistics.pstdev(toks),2) if len(toks)>1 else 0,\n",
    "        \"time_sec\": round(elapsed,2), \"redundancy_pct\": 0.0\n",
    "    })\n",
    "\n",
    "    # --- 5) LLM-BASED (Gemini) ---\n",
    "    cfg_name = \"llm_gemini_flash_t300\"\n",
    "    rows, t0 = [], time.time()\n",
    "    approx_in_tokens = 0; approx_out_tokens = 0\n",
    "    for d in tqdm(docs, desc=cfg_name):\n",
    "        url, section, text = d.get(\"url\",\"\"), d.get(\"section\",\"\"), d.get(\"text\",\"\")\n",
    "        if not text: continue\n",
    "        # keep prompt sizes manageable\n",
    "        clip = text if len(text) < 12000 else text[:12000]\n",
    "        approx_in_tokens += math.ceil(len(clip)/4)\n",
    "        chunks = gemini_chunk(clip, target_tokens=300, model=\"gemini-1.5-flash\")\n",
    "        for ch in chunks:\n",
    "            rows.append({\"strategy\":\"llm\",\"config\":cfg_name,\"url\":url,\"section\":section,\n",
    "                         \"text\":ch, \"tokens\":tok_count(ch)})\n",
    "            approx_out_tokens += math.ceil(len(ch)/4)\n",
    "    elapsed = time.time()-t0\n",
    "    save_jsonl(OUT_DIR / f\"chunks_{cfg_name}.jsonl\", rows)\n",
    "    toks = [r[\"tokens\"] for r in rows] if rows else []\n",
    "    ablation_rows.append({\n",
    "        \"strategy\":\"llm\",\"config\":cfg_name,\n",
    "        \"#chunks\": len(rows), \"tokens_total\": sum(toks) if toks else 0,\n",
    "        \"avg_tokens\": round(statistics.mean(toks),2) if toks else 0,\n",
    "        \"std_tokens\": round(statistics.pstdev(toks),2) if len(toks)>1 else 0,\n",
    "        \"time_sec\": round(elapsed,2),\n",
    "        \"redundancy_pct\": 0.0,\n",
    "        \"approx_in_tokens\": approx_in_tokens,\n",
    "        \"approx_out_tokens\": approx_out_tokens,\n",
    "        \"model\":\"gemini-1.5-flash\"\n",
    "    })\n",
    "\n",
    "    # Write ablation table\n",
    "    pd.DataFrame(ablation_rows).to_csv(ABLATION_CSV, index=False)\n",
    "    print(\"Saved:\", ABLATION_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "132d3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fixed_s256_o0:   0%|          | 0/16 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "fixed_s256_o0: 100%|██████████| 16/16 [00:00<00:00, 48.83it/s]\n",
      "fixed_s256_o64: 100%|██████████| 16/16 [00:00<00:00, 48.84it/s]\n",
      "fixed_s256_o128: 100%|██████████| 16/16 [00:00<00:00, 33.90it/s]\n",
      "fixed_s512_o0: 100%|██████████| 16/16 [00:00<00:00, 53.30it/s]\n",
      "fixed_s512_o64: 100%|██████████| 16/16 [00:00<00:00, 49.78it/s]\n",
      "fixed_s512_o128: 100%|██████████| 16/16 [00:00<00:00, 42.64it/s] \n",
      "fixed_s1024_o0: 100%|██████████| 16/16 [00:00<00:00, 54.52it/s]\n",
      "fixed_s1024_o64: 100%|██████████| 16/16 [00:00<00:00, 48.54it/s] \n",
      "fixed_s1024_o128: 100%|██████████| 16/16 [00:00<00:00, 53.08it/s]\n",
      "semantic_sentence_t512: 100%|██████████| 16/16 [00:01<00:00, 14.57it/s]\n",
      "semantic_paragraph_t512: 100%|██████████| 16/16 [00:00<00:00, 97.24it/s]\n",
      "structural_html: 100%|██████████| 16/16 [00:07<00:00,  2.18it/s]\n",
      "recursive_t512: 100%|██████████| 16/16 [00:07<00:00,  2.10it/s]\n",
      "llm_gemini_flash_t300: 100%|██████████| 16/16 [01:30<00:00,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini chunking failed: Extra data: line 6 column 1 (char 11927)\n",
      "Saved: C:\\Users\\harsh\\OneDrive\\Desktop\\LLM Assignment 2\\Chunking\\chunking_ablation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------- run --------------------\n",
    "run_all_chunking()\n",
    "\n",
    "# Cleanup Playwright if opened\n",
    "try:\n",
    "    if _pw is not None:\n",
    "        _pw.page.close()\n",
    "        _pw.browser.close()\n",
    "        _pw.stop()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bac07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weighted evaluation -> C:\\Users\\harsh\\OneDrive\\Desktop\\LLM Assignment 2\\Chunking\\chunking_ablation_weighted.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>config</th>\n",
       "      <th>#chunks</th>\n",
       "      <th>avg_tokens</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>semantic_coherence</th>\n",
       "      <th>completeness</th>\n",
       "      <th>size_band_pct</th>\n",
       "      <th>info_density</th>\n",
       "      <th>domain_grouping_nmi</th>\n",
       "      <th>throughput</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s512_o64</td>\n",
       "      <td>100</td>\n",
       "      <td>456.44</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.6297</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>19.1638</td>\n",
       "      <td>0.5119</td>\n",
       "      <td>303.030303</td>\n",
       "      <td>0.737413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s512_o0</td>\n",
       "      <td>88</td>\n",
       "      <td>459.28</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6042</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>19.0108</td>\n",
       "      <td>0.5389</td>\n",
       "      <td>293.333333</td>\n",
       "      <td>0.732031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s512_o128</td>\n",
       "      <td>114</td>\n",
       "      <td>463.24</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>16.6480</td>\n",
       "      <td>0.5022</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.721228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>semantic</td>\n",
       "      <td>semantic_paragraph_t512</td>\n",
       "      <td>16</td>\n",
       "      <td>2525.31</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>19.2189</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>0.624645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s256_o128</td>\n",
       "      <td>325</td>\n",
       "      <td>242.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.7561</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.1870</td>\n",
       "      <td>0.4431</td>\n",
       "      <td>677.083333</td>\n",
       "      <td>0.527702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s256_o64</td>\n",
       "      <td>217</td>\n",
       "      <td>245.44</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.6285</td>\n",
       "      <td>0.9885</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.1522</td>\n",
       "      <td>0.4213</td>\n",
       "      <td>657.575758</td>\n",
       "      <td>0.486846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s256_o0</td>\n",
       "      <td>168</td>\n",
       "      <td>240.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.5293</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>17.2019</td>\n",
       "      <td>0.4441</td>\n",
       "      <td>509.090909</td>\n",
       "      <td>0.464757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s1024_o128</td>\n",
       "      <td>53</td>\n",
       "      <td>846.79</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>0.6132</td>\n",
       "      <td>0.1698</td>\n",
       "      <td>17.2407</td>\n",
       "      <td>0.6379</td>\n",
       "      <td>176.666667</td>\n",
       "      <td>0.439918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s1024_o64</td>\n",
       "      <td>49</td>\n",
       "      <td>866.88</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.7063</td>\n",
       "      <td>0.5918</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>17.1009</td>\n",
       "      <td>0.6823</td>\n",
       "      <td>148.484848</td>\n",
       "      <td>0.426444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fixed</td>\n",
       "      <td>fixed_s1024_o0</td>\n",
       "      <td>46</td>\n",
       "      <td>878.46</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.7050</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>16.0205</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>153.333333</td>\n",
       "      <td>0.415829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llm</td>\n",
       "      <td>llm_gemini_flash_t300</td>\n",
       "      <td>47</td>\n",
       "      <td>192.40</td>\n",
       "      <td>90.46</td>\n",
       "      <td>0.3967</td>\n",
       "      <td>0.7701</td>\n",
       "      <td>0.1489</td>\n",
       "      <td>21.5512</td>\n",
       "      <td>0.5703</td>\n",
       "      <td>0.519567</td>\n",
       "      <td>0.414043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>semantic</td>\n",
       "      <td>semantic_sentence_t512</td>\n",
       "      <td>516</td>\n",
       "      <td>78.30</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.3863</td>\n",
       "      <td>0.6114</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>10.1978</td>\n",
       "      <td>0.4102</td>\n",
       "      <td>469.090909</td>\n",
       "      <td>0.268134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>recursive</td>\n",
       "      <td>recursive_t512</td>\n",
       "      <td>522</td>\n",
       "      <td>78.14</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.3887</td>\n",
       "      <td>0.6188</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>10.1844</td>\n",
       "      <td>0.3267</td>\n",
       "      <td>68.414155</td>\n",
       "      <td>0.196469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>structural</td>\n",
       "      <td>structural_html</td>\n",
       "      <td>4</td>\n",
       "      <td>6093.50</td>\n",
       "      <td>65.52</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2737</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.2308</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.061050</td>\n",
       "      <td>0.150408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      strategy                   config  #chunks  avg_tokens  time_sec  \\\n",
       "4        fixed           fixed_s512_o64      100      456.44      0.33   \n",
       "3        fixed            fixed_s512_o0       88      459.28      0.30   \n",
       "5        fixed          fixed_s512_o128      114      463.24      0.38   \n",
       "10    semantic  semantic_paragraph_t512       16     2525.31      0.17   \n",
       "2        fixed          fixed_s256_o128      325      242.51      0.48   \n",
       "1        fixed           fixed_s256_o64      217      245.44      0.33   \n",
       "0        fixed            fixed_s256_o0      168      240.67      0.33   \n",
       "8        fixed         fixed_s1024_o128       53      846.79      0.30   \n",
       "7        fixed          fixed_s1024_o64       49      866.88      0.33   \n",
       "6        fixed           fixed_s1024_o0       46      878.46      0.30   \n",
       "12         llm    llm_gemini_flash_t300       47      192.40     90.46   \n",
       "9     semantic   semantic_sentence_t512      516       78.30      1.10   \n",
       "11   recursive           recursive_t512      522       78.14      7.63   \n",
       "13  structural          structural_html        4     6093.50     65.52   \n",
       "\n",
       "    semantic_coherence  completeness  size_band_pct  info_density  \\\n",
       "4               0.6297        0.9850         0.8800       19.1638   \n",
       "3               0.6042        0.9659         0.8864       19.0108   \n",
       "5               0.6802        0.9912         0.8684       16.6480   \n",
       "10              1.0000        0.6875         0.3125       19.2189   \n",
       "2               0.7561        0.9831         0.0000       16.1870   \n",
       "1               0.6285        0.9885         0.0000       15.1522   \n",
       "0               0.5293        0.9762         0.0000       17.2019   \n",
       "8               0.6995        0.6132         0.1698       17.2407   \n",
       "7               0.7063        0.5918         0.1224       17.1009   \n",
       "6               0.7050        0.6087         0.1304       16.0205   \n",
       "12              0.3967        0.7701         0.1489       21.5512   \n",
       "9               0.3863        0.6114         0.0310       10.1978   \n",
       "11              0.3887        0.6188         0.0441       10.1844   \n",
       "13              0.0000        0.2737         0.0000       10.2308   \n",
       "\n",
       "    domain_grouping_nmi  throughput  weighted_score  \n",
       "4                0.5119  303.030303        0.737413  \n",
       "3                0.5389  293.333333        0.732031  \n",
       "5                0.5022  300.000000        0.721228  \n",
       "10               0.9003   94.117647        0.624645  \n",
       "2                0.4431  677.083333        0.527702  \n",
       "1                0.4213  657.575758        0.486846  \n",
       "0                0.4441  509.090909        0.464757  \n",
       "8                0.6379  176.666667        0.439918  \n",
       "7                0.6823  148.484848        0.426444  \n",
       "6                0.6440  153.333333        0.415829  \n",
       "12               0.5703    0.519567        0.414043  \n",
       "9                0.4102  469.090909        0.268134  \n",
       "11               0.3267   68.414155        0.196469  \n",
       "13               1.0000    0.061050        0.150408  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Extended Chunking Evaluation (Weighted Scoring) ===\n",
    "# Produces: data/chunking_ablation_weighted.csv and prints a ranked table.\n",
    "\n",
    "import json, re, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _load_chunks(pattern: str):\n",
    "    \"\"\"Load all chunks_*.jsonl files that match a simple substring pattern on filename.\"\"\"\n",
    "    rows = []\n",
    "    for p in OUT_DIR.glob(pattern):\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    row = json.loads(line)\n",
    "                    # expected fields: strategy, config, url, section, text, tokens\n",
    "                    rows.append(row)\n",
    "                except:\n",
    "                    pass\n",
    "    return rows\n",
    "\n",
    "def _tokenize_lower(s: str):\n",
    "    return re.findall(r\"[A-Za-z0-9]+\", (s or \"\").lower())\n",
    "\n",
    "def _tfidf_matrix(texts):\n",
    "    # light TF-IDF (no stopwords to keep domain terms)\n",
    "    vec = TfidfVectorizer(min_df=1)\n",
    "    X = vec.fit_transform(texts)\n",
    "    return X, vec\n",
    "\n",
    "def _normalize_01(series):\n",
    "    # avoid divide by zero\n",
    "    if isinstance(series, list):\n",
    "        series = np.array(series, dtype=float)\n",
    "    mn, mx = float(np.min(series)), float(np.max(series))\n",
    "    if mx - mn < 1e-9:\n",
    "        return np.ones_like(series) * 1.0\n",
    "    return (series - mn) / (mx - mn)\n",
    "\n",
    "# ---------- Load base corpus (for completeness) ----------\n",
    "docs = json.loads(SECTIONS_JSON.read_text(encoding=\"utf-8\"))\n",
    "# map url -> token set for coverage measure\n",
    "url_tokens = {}\n",
    "for d in docs:\n",
    "    url = d.get(\"url\",\"\")\n",
    "    text = d.get(\"text\",\"\")\n",
    "    url_tokens[url] = set(_tokenize_lower(text))\n",
    "\n",
    "# ---------- Domain keyword list (adjustable) ----------\n",
    "DOMAIN_TERMS = set(\"\"\"\n",
    "jio jiopay business merchant settlement refunds dispute chargeback kyc aml onboarding grievance\n",
    "billpay biller pos \"point of sale\" upi \"upi hub\" payment gateway checkout intent qr \"soundbox\" \"biller centre\"\n",
    "privacy policy terms conditions complaint resolution investor relations help center faq invoice reconciliation\n",
    "\"\"\".replace('\"','').split())\n",
    "\n",
    "# ---------- Load your original ablation (counts/size/time) ----------\n",
    "abl = pd.read_csv(ABLATION_CSV)\n",
    "\n",
    "# ---------- Build strategy->chunks map ----------\n",
    "# We detect available strategies/configs by scanning OUT_DIR files\n",
    "all_files = list(OUT_DIR.glob(\"chunks_*.jsonl\"))\n",
    "if not all_files:\n",
    "    raise SystemExit(\"No chunk files found in OUT_DIR. Run chunking first.\")\n",
    "\n",
    "# Load all chunks grouped by (strategy, config)\n",
    "grouped = defaultdict(list)\n",
    "for fp in all_files:\n",
    "    # filename: chunks_<config>.jsonl (we stored strategy inside rows)\n",
    "    with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                r = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "            strat = r.get(\"strategy\",\"unknown\")\n",
    "            cfg = r.get(\"config\",\"unknown\")\n",
    "            grouped[(strat, cfg)].append(r)\n",
    "\n",
    "# ---------- Metrics per (strategy, config) ----------\n",
    "rows = []\n",
    "for (strategy, config), chunks in grouped.items():\n",
    "    if not chunks:\n",
    "        continue\n",
    "\n",
    "    # --- Size-band Fit (300-600 tokens) ---\n",
    "    toks = np.array([c.get(\"tokens\",0) for c in chunks], dtype=int)\n",
    "    size_band_pct = float(( (toks >= 300) & (toks <= 600) ).sum()) / max(1, len(toks))\n",
    "\n",
    "    # --- Info Density: domain keyword hits per 100 tokens ---\n",
    "    dens_vals = []\n",
    "    for c in chunks:\n",
    "        T = c.get(\"text\",\"\")\n",
    "        toks_c = _tokenize_lower(T)\n",
    "        if not toks_c:\n",
    "            continue\n",
    "        hits = sum(1 for t in toks_c if t in DOMAIN_TERMS)\n",
    "        dens = 100.0 * hits / max(1, len(toks_c))\n",
    "        dens_vals.append(dens)\n",
    "    info_density = float(np.mean(dens_vals)) if dens_vals else 0.0\n",
    "\n",
    "    # --- Semantic Coherence (within each URL): mean max-neighbor similarity ---\n",
    "    # Compute TF-IDF per strategy-config across all its chunks\n",
    "    texts = [c.get(\"text\",\"\") for c in chunks]\n",
    "    X, vec = _tfidf_matrix(texts)\n",
    "    # index chunks by url to compare neighbors within same source doc\n",
    "    by_url_idx = defaultdict(list)\n",
    "    for i, c in enumerate(chunks):\n",
    "        by_url_idx[c.get(\"url\",\"\")].append(i)\n",
    "\n",
    "    sim_scores = []\n",
    "    for u, idxs in by_url_idx.items():\n",
    "        if len(idxs) < 2:\n",
    "            continue\n",
    "        Xi = X[idxs]\n",
    "        sims = cosine_similarity(Xi)\n",
    "        # for each row, take top-1 neighbor similarity (excluding self)\n",
    "        for i in range(sims.shape[0]):\n",
    "            row = sims[i].copy()\n",
    "            row[i] = -1.0\n",
    "            sim_scores.append(float(np.max(row)))\n",
    "    semantic_coherence = float(np.mean(sim_scores)) if sim_scores else 0.0\n",
    "\n",
    "    # --- Completeness: token coverage vs original url text + continuity penalty ---\n",
    "    cover_scores = []\n",
    "    small_or_huge = 0\n",
    "    for u, idxs in by_url_idx.items():\n",
    "        # coverage = |union(chunk_tokens)| / |doc_tokens|\n",
    "        doc_tok = url_tokens.get(u, set())\n",
    "        if not doc_tok:\n",
    "            continue\n",
    "        union = set()\n",
    "        for i in idxs:\n",
    "            union |= set(_tokenize_lower(texts[i]))\n",
    "        cov = float(len(union & doc_tok)) / max(1, len(doc_tok))\n",
    "        cover_scores.append(cov)\n",
    "\n",
    "    # continuity penalty: fraction of chunks <80 or >800 tokens\n",
    "    small_or_huge = float(((toks < 80) | (toks > 800)).sum()) / max(1, len(toks))\n",
    "    completeness = 0.5 * (np.mean(cover_scores) if cover_scores else 0.0) + 0.5 * (1.0 - small_or_huge)\n",
    "\n",
    "    # --- Domain grouping quality: cluster vs section label alignment (NMI) ---\n",
    "    # K = number of unique sections but cap to [2, 12]\n",
    "    sections = [ (c.get(\"section\") or \"\").strip() or \"NA\" for c in chunks ]\n",
    "    uniq_sections = [s for s, _ in Counter(sections).most_common()]\n",
    "    K = min(max(len(uniq_sections), 2), 12)\n",
    "    try:\n",
    "        km = KMeans(n_clusters=K, n_init=\"auto\", random_state=42)\n",
    "        labels = km.fit_predict(X)\n",
    "        nmi = normalized_mutual_info_score(sections, labels)\n",
    "    except Exception:\n",
    "        nmi = 0.0\n",
    "\n",
    "    rows.append({\n",
    "        \"strategy\": strategy,\n",
    "        \"config\": config,\n",
    "        \"size_band_pct\": round(size_band_pct, 4),\n",
    "        \"info_density\": round(info_density, 4),\n",
    "        \"semantic_coherence\": round(semantic_coherence, 4),\n",
    "        \"completeness\": round(completeness, 4),\n",
    "        \"domain_grouping_nmi\": round(nmi, 4),\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(rows)\n",
    "\n",
    "# ---------- Merge with base ablation (for time/chunk counts) ----------\n",
    "# Expect columns in abl: strategy, config, #chunks, tokens_total, avg_tokens, std_tokens, time_sec, redundancy_pct\n",
    "merged = abl.merge(eval_df, on=[\"strategy\",\"config\"], how=\"left\")\n",
    "\n",
    "# ---------- Performance & weighted score ----------\n",
    "# Throughput = #chunks / time_sec (avoid 0)\n",
    "merged[\"throughput\"] = merged.apply(lambda r: (r[\"#chunks\"] / r[\"time_sec\"]) if (r.get(\"time_sec\",0)>0) else 0.0, axis=1)\n",
    "\n",
    "# Normalize metrics to [0,1] per column where higher is better\n",
    "for col in [\"semantic_coherence\",\"completeness\",\"size_band_pct\",\"info_density\",\"domain_grouping_nmi\",\"throughput\"]:\n",
    "    merged[f\"{col}_norm\"] = _normalize_01(merged[col].fillna(0.0).to_numpy())\n",
    "\n",
    "# Weighted rubric (like your classmate’s)\n",
    "# Retrieval Quality (40%) = 20% semantic_coherence + 20% completeness\n",
    "retrieval_quality = 0.20*merged[\"semantic_coherence_norm\"] + 0.20*merged[\"completeness_norm\"]\n",
    "\n",
    "# Size Optimization (25%) = size_band_pct\n",
    "size_optimization = 0.25*merged[\"size_band_pct_norm\"]\n",
    "\n",
    "# Domain-Specific (25%) = 10% info_density + 15% domain_grouping_nmi\n",
    "domain_specific = 0.10*merged[\"info_density_norm\"] + 0.15*merged[\"domain_grouping_nmi_norm\"]\n",
    "\n",
    "# Performance (10%) = throughput\n",
    "performance = 0.10*merged[\"throughput_norm\"]\n",
    "\n",
    "merged[\"weighted_score\"] = retrieval_quality + size_optimization + domain_specific + performance\n",
    "merged = merged.sort_values(\"weighted_score\", ascending=False)\n",
    "\n",
    "# Save & display\n",
    "weighted_csv = ABLATION_CSV.parent / \"chunking_ablation_weighted.csv\"\n",
    "merged.to_csv(weighted_csv, index=False)\n",
    "\n",
    "print(\"Saved weighted evaluation ->\", weighted_csv)\n",
    "display(merged[[\n",
    "    \"strategy\",\"config\",\"#chunks\",\"avg_tokens\",\"time_sec\",\n",
    "    \"semantic_coherence\",\"completeness\",\"size_band_pct\",\"info_density\",\"domain_grouping_nmi\",\"throughput\",\n",
    "    \"weighted_score\"\n",
    "]].head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
